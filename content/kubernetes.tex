\chapter{Kubernetes integráció}

\section{Bevezetés}

A konténerizáció nagy előnyt nyújt, mivel szabványosított, elszigetelt környezetet kínál a szoftverek futtatásához. 
A Docker Compose elterjedt a helyi, több konténert tartalmazó alkalmazásokhoz, egyszerűsítve az összekapcsolt 
szolgáltatások definiálását és futtatását. Mivel azonban sokszor skálázódásra van szükség, és olyan funkciókra, 
mint a nagy rendelkezésre állás, az automatikus skálázás és a kifinomult orkesztráció, a Kubernetes vált a konténer 
orkesztráció szabványává.

Ebben a fejezetben megmutatom, hogy az eredetileg a Docker Compose segítségével definiált rendszeremet, 
hogyan migráltam Kubernetes környezetbe. A rendszeremben a már meglévő szolgáltatások jelenek meg, mint a 
Prometheus a felügyelethez, a Grafana a vizualizációhoz, több szimulátorszolgáltatás és egy vezérlőszerver. 
Itt bemutatom a Docker Compose konfigurációk Kubernetes manifesztekbe való átforgatásának kihívásait.

\section{A Docker Compose és Kubernetes áttekintése}

\paragraph{Docker Compose}
Ezzel több konténert tartalmazó Docker alkalmazásokat lehet definiálni és futtatni. 
Konfigurációja egy YML fájlban tárolt, ahol a szolgáltatásokat, hálózatokati kapcsolatokat, köteteket és 
függőségeket lehet megadni. A Docker Compose leegyszerűsíti a konténerek egyetlen hoszton történő orkesztrációját, 
így segíti a fejlesztést és tesztelést.

\paragraph{Kubernetes}
A Kubernetes (K8) viszont egy robusztus, open source platform a konténerek telepítésének, 
skálázásának és üzemeltetésének automatizálására hostokon. A Kubernetes új absztrakciókat vezet be:

\begin{itemize}
    \item \textbf{Pod:} Ez a legkisebb telepíthető egység, amely egy vagy több konténert foglalnak magukba.
    
    \item \textbf{Deployment:} Állapot nélküli alkalmazások kezelésére szolgáló objektumok, amelyek olyan funkciókat kínálnak, mint a gördülő frissítések és a visszaállítás.
    
    \item \textbf{Service:} Végpontokat biztosítanak a podok eléréséhez, segítve a felfedezést és a terheléselosztást.
    
    \item \textbf{ConfigMap és Secret:} Mechanizmus a konfiguráció és az imagek szétválasztására.
    
    \item \textbf{PersistentVolumeClaim (PVC):} Absztrakció adattárolásra.
\end{itemize}

A migráció során ezeket képeztem le docker-ből k8-ba.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth, keepaspectratio]{figures/components-of-kubernetes.jpeg}
    \caption{Kubernetes architektúra \cite{wallarm_kubernetes_cluster}} 
\end{figure}

\section{Rendszerarchitektúrája}

A rendszer a már megismert következő részeket tartalmazza:

\begin{itemize}
    \item \textbf{Prometheus:} Egyéni prometheus.yml fájllal konfigurált idősoros adatbázis. 
    Ennek szerencsétlensége, hogy az újra konfiguráció csak újra indítással lehetséges.
    
    \item \textbf{Grafana:} Vizualizációs eszköz, ami közvetlen a Prometheushoz kapcsolódik megjelenítéséhez.
    
    \item \textbf{ESP8266 szimulátorok:} Itt épen három példány szimulálja a különböző szimulátorazonosítókkal 
    rendelkező eszközöket.
    
    \item \textbf{Breaker Simulators:} Más jellegű, de hasonló célú szimulátor.
    
    \item \textbf{Vezérlőszerver:} Lebonyolítja az eszközök közötti interakciókat, vezérlést és adatok továbbítását.
    
    \item \textbf{System Simulator:} A rendszer általános viselkedését emuláló központi szolgáltatás.
\end{itemize}

A Docker Compose alkalmazásban ezek az összetevők hálózaton és socketeken keresztül kapcsolódtak össze, 
és meghatározott végpontokon jelenítettek meg. \cite{docker_kubernetes}

\section{A Docker Compose beállítások konvertálása Kubernetes manifesztekké}

A Docker Compose-ról a Kubernetesre való áttérés magában foglalja az alkalmazás architektúrájának 
újragondolását a podok, deployement-ek, szolgáltatások és más Kubernetes objektumok szerint. \cite{kubernetes}

\subsection{Névtér- és konfigurációkezelés}

Itt létrehoztam egy névteret (pl. monitoring) ez izolációt biztosít az alkalmazás számára. 
A ConfigMap a Prometheus konfiguráció tárolására szolgál (a prometheus.yml tartalma), 
lehetővé téve a konfiguráció frissítését a konténerek image-einek újbóli legenerálása nélkül.

\begin{lstlisting}
    apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |-
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
\end{lstlisting}

\subsection{Deployment-ek és Service-ek}

Minden szolgáltatás Docker Compose-ban egy Deployment és egy Service formájában jelenik meg a Kubernetesben. 
A Deployment kezeli az alkalmazásban a podokat, a Service ezeket a podokat teszi elérhetővé.

Például a Prometheus szolgáltatás egyetlen replikával rendelkezik. Konfigurációja a ConfigMap-ról van mountolva, 
a perzisztens adatai pedig egy PersistentVolumeClaim (PVC) segítségével tárolom. Hasonlóképpen, más szolgáltatások, 
például az ESP8266 szimulátorok és a vezérlő szerver deployement-ekké alakulnak át, amelyek környezeti változókat 
és portkonfigurációkat adnak meg.

\subsection{Perzisztens tárolók kezelése}

A Docker Compose-ban gyakran definiálnak volume-okat az adattárolására. A Kubernetesben ezt a 
PersistentVolumeClaims biztosítja. A készített rendszeremben a Prometheus, mind a Grafana perzisztens 
tárolót igényelt az adatok megőrzéséhez, amiket a PVC-k létrehozásával és konténerekhez kötésével értem el.

\begin{lstlisting}
  apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-data
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
\end{lstlisting}

\subsection{Szolgáltatások elérhetővé tétele és hálózati konfiguráció}

A Docker Compose-ban a portok hozzárendelését a konfigurációban végezzük. 
A Kubernetesben a portok meghatározást a Service-ek kezelik, ezek lehetnek 
NodePort típusúak a külső hozzáféréshez vagy ClusterIP típusúak a belső kommunikációhoz. 
A migráció során a konténerek portjait le kellett képezni a hosztokra, hogy a külső interfész 
ugyanaz maradjon az eredeti Docker Compose-hoz képest.

Például a Docker Compose-ban az 5000-es porton található vezérlő szervert egy olyan Kubernetes Service replikálja, 
amely egy adott NodePort-ot rendel hozzá, például 30050-et.

\subsection{Telepítés és tesztelés}

A Kubernetes manifeszt a kubectl apply -f paranccsal kerül alkalmazásra. Ez telepíti az összes komponenst a névtérben. 
A telepítés után a szabványos Kubernetes-parancsok (pl. kubectl get pods, kubectl logs, kubectl describe) 
a podok állapotának ellenőrzésére szolgálnak. Így iteratívan lehet tesztelni az új rendszert és később 
szolgáltatás kimaradás nélkül frissíteni.

Telepítéséhez a következő parancsot használjuk:

\begin{lstlisting}
  kubectl apply -f monitoring.yaml
\end{lstlisting}

És hogy megvizsgáljuk a telepített podokat:

\begin{lstlisting}
  kubectl get pods -n monitoring
\end{lstlisting}

\section{Nagy elérhetőségű rendszer implementációja}

Az aktív elsődleges és passzív készenléti minta csökkenti a komplexitást és 
emellett megbízható felügyeletet biztosít:

\begin{itemize}
  \item A Prometheus folyamatos adatreprodukciója mindent megőriz a második helyszínen.
  
  \item A replikán keresztül biztosított a Grafana-B azonnali használhatósága.
  
  \item Az átállást csak a DNS/szolgáltatás frissítési sebessége korlátozza.
  
  \item Ez a topológia megfelel a megbízhatósági céloknak a monitorozási környezetbe.
  
\end{itemize}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth, keepaspectratio]{figures/kubernetes.png}
  \caption{Hibrid kubernetes topológia}
\end{figure}

A Prometheus-A minden célpontot lekérdez, és elvégzi az összes értékelést.
A Prometheus-B távoli írást kap A-tól (A hálózat felesleges terhelésének 
elkerülése érdekében nem scrapel közvetlen).
A Grafana-B csatlakozik a Prometheus-B-hez, és a dashboardokat inen frissíti 
(ez közvetlenül nem érhető el).
Egyetlen DNS név mutat az A ingressre. A Kubernetes és egy külső állapotellenőrzés 
frissíti a DNS-t a B oldalra, amikor az A leáll.

\subsection{Replikák megvalósítása}

A VM-n a prometheus konfigurációja a következő képen történik.

\begin{lstlisting}
  apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-b
  namespace: monitoring
spec:
  replicas: 1
  selector: { matchLabels: { app: prometheus-b } }
  template:
    metadata: { labels: { app: prometheus-b } }
    spec:
      nodeSelector: { site: "b" }
      containers:
      - name: prometheus
        image: prom/prometheus:v2.49
        args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --web.enable-lifecycle
        volumeMounts:
          - name: data
            mountPath: /prometheus
        readinessProbe: { httpGet: { path: /-/ready, port: 9090 } }
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: prometheus-b-data
---
kind: PersistentVolumeClaim
metadata:
  name: prometheus-b-data
  namespace: monitoring
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: cloud-ssd
  resources: { requests: { storage: 50Gi } }
\end{lstlisting}

Az eredeti A prometheus-ból pedig a B-be folyamatosan írunk.

\begin{lstlisting}
  remote_write:
- url: http://prometheus-b.monitoring.svc.cluster.local:9090/api/v1/write
  queue_config:
    capacity: 10000
    max_shards: 5
    max_samples_per_send: 1000
    batch_send_deadline: 5s
\end{lstlisting}

A grafana megvalósítása során igazából csak egy ugyanolyan deployement-et hozunk létre.
Ez egy másolat a másikról amire ha kell áttudunk bármikor térni.

\begin{lstlisting}
  spec:
  replicas: 1
  template:
    metadata: { labels: { app: grafana-b } }
    spec:
      nodeSelector: { site: "b" }
      containers:
      - name: grafana
        image: grafana/grafana:11.0.0
        env:
          - name: GF_DATABASE_URL     # same secret as primary
            valueFrom: { secretKeyRef: { name: grafana-db, key: db_url } }
          - name: GF_SECURITY_SECRET_KEY
            valueFrom: { secretKeyRef: { name: grafana-db, key: secret } }
        readinessProbe:
          httpGet: { path: /api/health, port: 3000 }
\end{lstlisting}

\subsection{KubeADM}

A projektemben a kubeadm-re támaszkodtam, hogy kubernetes klasztert készítsek a linux vm-et bevonva. 
Ez megkönnyítette a folyamatot mert magasabb színtű tervezésre volt csak szükség és ez megoldotta magától 
az alacsonyabb szintű problémákat.

\begin{lstlisting}
  sudo kubeadm init --config=/etc/kubeadm/config.yaml
\end{lstlisting}

Az inicializálás után csak egy tokent kellett adni a nodenak, hogy csatlakozzon a clusterhez.
Ezután a a további folyamatokat kezelte is a Kubeadm.

\begin{lstlisting}
  sudo kubeadm join 10.200.0.1:6443 \
    --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>
\end{lstlisting}

Ennek köszönhetően egy hasonló rendszerben, ha a egy node meghibásodik akkor a másik átveszi a helyét és felhasználói oldalról
nem érzünk kiesést. A helyre állítás során, pedig csak egy parancsot kell kiadnunk:

\begin{lstlisting}
  kubeadm join
\end{lstlisting}

Ezután újra csatlakoztattuk is a node-ot és újonnan felépíthetjük a clusterben.